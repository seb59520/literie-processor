# SystÃ¨me de Tests AutomatisÃ©s - Application Matelas

## ğŸ“‹ Vue d'ensemble

Ce document dÃ©crit le systÃ¨me complet de tests automatisÃ©s pour l'application Matelas de LITERIE WESTEYLYNCK. Le systÃ¨me comprend des tests unitaires, d'intÃ©gration, de performance et de rÃ©gression.

## ğŸ—ï¸ Architecture des Tests

```
tests/
â”œâ”€â”€ __init__.py                 # Package de tests
â”œâ”€â”€ conftest.py                 # Configuration pytest et fixtures
â”œâ”€â”€ test_unitaires.py           # Tests unitaires complets
â”œâ”€â”€ test_integration.py         # Tests d'intÃ©gration
â”œâ”€â”€ test_performance.py         # Tests de performance
â”œâ”€â”€ test_regression.py          # Tests de rÃ©gression
â”œâ”€â”€ run_all_tests.py            # Script d'exÃ©cution principal
â””â”€â”€ requirements_test.txt       # DÃ©pendances pour les tests
```

## ğŸš€ Installation et Configuration

### 1. Installation des dÃ©pendances

```bash
# Installer les dÃ©pendances de test
pip install -r tests/requirements_test.txt

# Ou utiliser le script d'installation
python tests/run_all_tests.py --install-deps
```

### 2. Configuration

Le fichier `tests/conftest.py` contient toutes les configurations et fixtures communes :
- RÃ©pertoires temporaires
- DonnÃ©es de test standardisÃ©es
- Mocks pour Excel et LLM
- Seuils de performance
- Cas de test de rÃ©gression

## ğŸ§ª Types de Tests

### 1. Tests Unitaires (`test_unitaires.py`)

**Objectif** : Tester chaque fonction individuellement

**Modules testÃ©s** :
- `client_utils` : Extraction et validation des donnÃ©es client
- `dimensions_utils` : DÃ©tection des dimensions
- `hauteur_utils` : Calcul de hauteur
- `fermete_utils` : DÃ©tection de fermetÃ©
- `housse_utils` : DÃ©tection du type de housse
- `matiere_housse_utils` : DÃ©tection de la matiÃ¨re
- `poignees_utils` : DÃ©tection des poignÃ©es
- `matelas_utils` : DÃ©tection du noyau
- `pre_import_utils` : CrÃ©ation et validation du prÃ©-import
- `excel_import_utils` : Import Excel
- `config` : Gestion de la configuration
- RÃ©fÃ©rentiels : Consultation des prix

**Exemple d'utilisation** :
```bash
# Tests unitaires avec dÃ©tails
pytest tests/test_unitaires.py -v

# Tests unitaires avec couverture
pytest tests/test_unitaires.py --cov=backend --cov-report=html
```

### 2. Tests d'IntÃ©gration (`test_integration.py`)

**Objectif** : Tester le flux complet de l'application

**FonctionnalitÃ©s testÃ©es** :
- Traitement complet de fichiers PDF
- IntÃ©gration LLM (avec et sans enrichissement)
- CrÃ©ation de prÃ©-import et export Excel
- Traitement de multiples fichiers
- Gestion d'erreurs
- Performance avec grand volume

**Exemple d'utilisation** :
```bash
# Tests d'intÃ©gration
pytest tests/test_integration.py -v

# Tests d'intÃ©gration avec mocks
pytest tests/test_integration.py -v --tb=short
```

### 3. Tests de Performance (`test_performance.py`)

**Objectif** : Mesurer et valider les performances

**MÃ©triques mesurÃ©es** :
- Temps de traitement
- Utilisation mÃ©moire
- Performance avec grand volume de donnÃ©es
- Benchmark complet

**Seuils de performance** :
- Extraction client : < 2s pour 1000 extractions
- DÃ©tection dimensions : < 1s pour 1000 dÃ©tections
- CrÃ©ation prÃ©-import : < 10s pour 500 configurations
- Import Excel : < 5s pour 200 configurations
- Traitement complet : < 30s pour 10 fichiers

**Exemple d'utilisation** :
```bash
# Tests de performance
pytest tests/test_performance.py -v

# Benchmark complet
python tests/test_performance.py
```

### 4. Tests de RÃ©gression (`test_regression.py`)

**Objectif** : DÃ©tecter les rÃ©gressions dans les fonctionnalitÃ©s

**FonctionnalitÃ©s surveillÃ©es** :
- Extraction de donnÃ©es client
- DÃ©tection de dimensions
- Calcul de hauteur
- DÃ©tection de fermetÃ©
- CrÃ©ation de prÃ©-import
- Validation des donnÃ©es
- Consultation des rÃ©fÃ©rentiels
- Import Excel
- Configuration

**Exemple d'utilisation** :
```bash
# Tests de rÃ©gression
pytest tests/test_regression.py -v

# Suite complÃ¨te de rÃ©gression
python tests/test_regression.py
```

## ğŸ¯ ExÃ©cution des Tests

### Script Principal

Le script `run_all_tests.py` permet d'exÃ©cuter tous les types de tests :

```bash
# Tous les tests
python tests/run_all_tests.py --all

# Tests unitaires avec dÃ©tails
python tests/run_all_tests.py --unit --verbose

# Tests de performance
python tests/run_all_tests.py --performance

# Tests de rÃ©gression
python tests/run_all_tests.py --regression

# Avec couverture de code
python tests/run_all_tests.py --all --coverage

# GÃ©nÃ©rer un rapport
python tests/run_all_tests.py --all --report
```

### Options Disponibles

- `--all` : ExÃ©cute tous les tests
- `--unit` : Tests unitaires uniquement
- `--integration` : Tests d'intÃ©gration uniquement
- `--performance` : Tests de performance uniquement
- `--regression` : Tests de rÃ©gression uniquement
- `--benchmark` : Benchmark de performance
- `--verbose` : Mode verbeux
- `--coverage` : GÃ©nÃ¨re un rapport de couverture
- `--report` : GÃ©nÃ¨re un rapport JSON
- `--install-deps` : Installe les dÃ©pendances

### ExÃ©cution Directe avec Pytest

```bash
# Tous les tests
pytest tests/ -v

# Tests unitaires
pytest tests/test_unitaires.py -v

# Tests avec marqueurs
pytest tests/ -m "unit" -v
pytest tests/ -m "performance" -v
pytest tests/ -m "regression" -v

# Tests avec couverture
pytest tests/ --cov=backend --cov=config --cov-report=html

# Tests parallÃ¨les
pytest tests/ -n auto
```

## ğŸ“Š Rapports et MÃ©triques

### Rapports de Couverture

```bash
# GÃ©nÃ©rer un rapport HTML
pytest tests/ --cov=backend --cov-report=html:test_reports/coverage_html

# GÃ©nÃ©rer un rapport XML
pytest tests/ --cov=backend --cov-report=xml:test_reports/coverage.xml

# Afficher les lignes manquantes
pytest tests/ --cov=backend --cov-report=term-missing
```

### Rapports de Performance

Les tests de performance gÃ©nÃ¨rent automatiquement des rapports avec :
- Temps d'exÃ©cution
- Utilisation mÃ©moire
- Comparaison avec les seuils
- Graphiques de performance

### Rapports JSON

Le script principal gÃ©nÃ¨re des rapports JSON dÃ©taillÃ©s dans `test_reports/` :
- RÃ©sumÃ© des tests
- DurÃ©e d'exÃ©cution
- Codes de retour
- Sorties et erreurs

## ğŸ”§ Configuration AvancÃ©e

### Fixtures PersonnalisÃ©es

Les fixtures dans `conftest.py` peuvent Ãªtre Ã©tendues :

```python
@pytest.fixture(scope="function")
def custom_test_data():
    """Fixture personnalisÃ©e pour des donnÃ©es de test"""
    return {
        "custom_field": "custom_value"
    }
```

### Marqueurs Pytest

```bash
# ExÃ©cuter les tests lents
pytest tests/ -m "slow" -v

# Exclure les tests de performance
pytest tests/ -m "not performance" -v

# Combiner les marqueurs
pytest tests/ -m "unit and not slow" -v
```

### Configuration des Seuils

Modifier les seuils de performance dans `conftest.py` :

```python
@pytest.fixture(scope="function")
def performance_thresholds():
    return {
        "extraction_client": {"time": 1.5, "memory": 80},  # Seuils ajustÃ©s
        # ...
    }
```

## ğŸ› DÃ©bogage des Tests

### Mode DÃ©bogage

```bash
# ArrÃªter au premier Ã©chec
pytest tests/ -x

# Afficher les variables locales en cas d'Ã©chec
pytest tests/ -l

# Mode traceback complet
pytest tests/ --tb=long

# Mode interactif
pytest tests/ --pdb
```

### Logs DÃ©taillÃ©s

```bash
# Logs de debug
pytest tests/ --log-cli-level=DEBUG

# Logs dans un fichier
pytest tests/ --log-file=test.log --log-file-level=DEBUG
```

### Tests SpÃ©cifiques

```bash
# Test spÃ©cifique
pytest tests/test_unitaires.py::TestClientUtils::test_extraire_ville_adresse -v

# Classe de tests spÃ©cifique
pytest tests/test_unitaires.py::TestClientUtils -v

# Tests avec pattern
pytest tests/ -k "client" -v
```

## ğŸ”„ IntÃ©gration Continue

### GitHub Actions

Exemple de workflow GitHub Actions :

```yaml
name: Tests
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r tests/requirements_test.txt
    - name: Run tests
      run: python tests/run_all_tests.py --all --coverage --report
    - name: Upload coverage
      uses: codecov/codecov-action@v1
```

### Jenkins Pipeline

```groovy
pipeline {
    agent any
    stages {
        stage('Install') {
            steps {
                sh 'pip install -r requirements.txt'
                sh 'pip install -r tests/requirements_test.txt'
            }
        }
        stage('Test') {
            steps {
                sh 'python tests/run_all_tests.py --all --coverage --report'
            }
        }
        stage('Report') {
            steps {
                publishHTML([
                    allowMissing: false,
                    alwaysLinkToLastBuild: true,
                    keepAll: true,
                    reportDir: 'test_reports/coverage_html',
                    reportFiles: 'index.html',
                    reportName: 'Coverage Report'
                ])
            }
        }
    }
}
```

## ğŸ“ˆ MÃ©triques et KPIs

### MÃ©triques de QualitÃ©

- **Couverture de code** : Objectif > 80%
- **Temps d'exÃ©cution** : < 5 minutes pour tous les tests
- **Taux de rÃ©ussite** : Objectif 100%
- **Tests de rÃ©gression** : 0 rÃ©gression dÃ©tectÃ©e

### MÃ©triques de Performance

- **Temps de traitement** : Respect des seuils dÃ©finis
- **Utilisation mÃ©moire** : < 500 MB pour les tests complets
- **ScalabilitÃ©** : Tests avec grand volume de donnÃ©es

## ğŸ› ï¸ Maintenance

### Ajout de Nouveaux Tests

1. **Tests unitaires** : Ajouter dans `test_unitaires.py`
2. **Tests d'intÃ©gration** : Ajouter dans `test_integration.py`
3. **Tests de performance** : Ajouter dans `test_performance.py`
4. **Tests de rÃ©gression** : Ajouter dans `test_regression.py`

### Mise Ã  Jour des Fixtures

Modifier `conftest.py` pour ajouter de nouvelles fixtures communes.

### Mise Ã  Jour des Seuils

Ajuster les seuils de performance selon l'Ã©volution de l'application.

## ğŸ“š Ressources

- [Documentation Pytest](https://docs.pytest.org/)
- [Pytest-cov Documentation](https://pytest-cov.readthedocs.io/)
- [Pytest-asyncio Documentation](https://pytest-asyncio.readthedocs.io/)
- [Meilleures pratiques de test](https://realpython.com/python-testing/)

## ğŸ¤ Contribution

Pour contribuer aux tests :

1. Suivre les conventions de nommage
2. Ajouter des docstrings pour tous les tests
3. Utiliser les fixtures existantes
4. Respecter les seuils de performance
5. Mettre Ã  jour la documentation

---

**Note** : Ce systÃ¨me de tests garantit la qualitÃ© et la fiabilitÃ© de l'application Matelas en dÃ©tectant automatiquement les rÃ©gressions et en validant les performances. 