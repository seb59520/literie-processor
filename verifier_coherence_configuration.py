#!/usr/bin/env python3
"""
V√©rification de la coh√©rence entre config.py et matelas_config.json
"""

import json
import os
import sys

def verifier_coherence_configuration():
    """V√©rifie la coh√©rence entre les diff√©rents syst√®mes de configuration"""
    
    print("üîç V√âRIFICATION DE LA COH√âRENCE DES CONFIGURATIONS")
    print("=" * 60)
    
    # 1. V√©rifier matelas_config.json
    print("\nüìÅ 1. CONFIGURATION MATELAS_CONFIG.JSON:")
    
    matelas_config_file = 'matelas_config.json'
    if os.path.exists(matelas_config_file):
        try:
            with open(matelas_config_file, 'r', encoding='utf-8') as f:
                matelas_config = json.load(f)
            
            print(f"   ‚úÖ {matelas_config_file} trouv√©")
            print(f"   üîß Provider LLM: {matelas_config.get('llm_provider', 'NON CONFIGUR√â')}")
            print(f"   üîß Provider actuel: {matelas_config.get('current_llm_provider', 'NON CONFIGUR√â')}")
            
            if 'ollama' in matelas_config:
                ollama_config = matelas_config['ollama']
                print(f"   üéØ Mod√®le Ollama: {ollama_config.get('model', 'NON CONFIGUR√â')}")
                print(f"   üåê URL: {ollama_config.get('base_url', 'NON CONFIGUR√â')}")
                print(f"   ‚è±Ô∏è Timeout: {ollama_config.get('timeout', 'NON CONFIGUR√â')}")
                
        except Exception as e:
            print(f"   ‚ùå Erreur lecture {matelas_config_file}: {e}")
            matelas_config = {}
    else:
        print(f"   ‚ùå {matelas_config_file} non trouv√©")
        matelas_config = {}
    
    # 2. V√©rifier config.py
    print("\nüìÅ 2. CONFIGURATION CONFIG.PY:")
    
    config_file = 'config.py'
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Chercher les valeurs par d√©faut
            if 'current_llm_provider.*openrouter' in content:
                print("   ‚ö†Ô∏è config.py a 'openrouter' comme provider par d√©faut")
            else:
                print("   ‚úÖ config.py n'a pas 'openrouter' comme d√©faut")
                
            if 'get_current_llm_provider' in content:
                print("   ‚úÖ config.py contient get_current_llm_provider")
                
                # Extraire la ligne avec le d√©faut
                lines = content.split('\n')
                for i, line in enumerate(lines):
                    if 'get_current_llm_provider' in line and 'openrouter' in line:
                        print(f"      üìç Ligne {i+1}: {line.strip()}")
                        
        except Exception as e:
            print(f"   ‚ùå Erreur lecture {config_file}: {e}")
    else:
        print(f"   ‚ùå {config_file} non trouv√©")
    
    # 3. V√©rifier backend/main.py
    print("\nüìÅ 3. CONFIGURATION BACKEND/MAIN.PY:")
    
    main_file = 'backend/main.py'
    if os.path.exists(main_file):
        try:
            with open(main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if 'gpt-oss:20b' in content:
                print("   ‚úÖ backend/main.py contient gpt-oss:20b")
            else:
                print("   ‚ùå backend/main.py ne contient pas gpt-oss:20b")
                
            if 'mistral:latest' in content:
                print("   ‚ö†Ô∏è backend/main.py contient encore mistral:latest")
            else:
                print("   ‚úÖ backend/main.py ne contient plus mistral:latest")
                
        except Exception as e:
            print(f"   ‚ùå Erreur lecture {main_file}: {e}")
    else:
        print(f"   ‚ùå {main_file} non trouv√©")
    
    # 4. V√©rifier backend/llm_provider.py
    print("\nüìÅ 4. CONFIGURATION BACKEND/LLM_PROVIDER.PY:")
    
    provider_file = 'backend/llm_provider.py'
    if os.path.exists(provider_file):
        try:
            with open(provider_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if 'gpt-oss:20b' in content:
                print("   ‚úÖ backend/llm_provider.py contient gpt-oss:20b")
            else:
                print("   ‚ùå backend/llm_provider.py ne contient pas gpt-oss:20b")
                
            if 'mistral:latest' in content:
                print("   ‚ö†Ô∏è backend/llm_provider.py contient encore mistral:latest")
            else:
                print("   ‚úÖ backend/llm_provider.py ne contient plus mistral:latest")
                
        except Exception as e:
            print(f"   ‚ùå Erreur lecture {provider_file}: {e}")
    else:
        print(f"   ‚ùå {provider_file} non trouv√©")
    
    # 5. Analyse des incoh√©rences
    print("\nüö® 5. ANALYSE DES INCOH√âRENCES:")
    
    incoh√©rences = []
    
    # V√©rifier la coh√©rence des providers
    matelas_provider = matelas_config.get('llm_provider', '')
    matelas_current = matelas_config.get('current_llm_provider', '')
    
    if matelas_provider != matelas_current:
        incoh√©rences.append(f"Provider LLM ({matelas_provider}) ‚â† Provider actuel ({matelas_current})")
    
    if matelas_provider == 'ollama' and matelas_current == 'ollama':
        print("   ‚úÖ Coh√©rence des providers : ollama")
    else:
        incoh√©rences.append(f"Provider incoh√©rent : {matelas_provider} vs {matelas_current}")
    
    # V√©rifier la coh√©rence des mod√®les
    if 'ollama' in matelas_config:
        ollama_model = matelas_config['ollama'].get('model', '')
        if ollama_model == 'gpt-oss:20b':
            print("   ‚úÖ Mod√®le Ollama coh√©rent : gpt-oss:20b")
        else:
            incoh√©rences.append(f"Mod√®le Ollama incoh√©rent : {ollama_model}")
    
    # Afficher les incoh√©rences
    if incoh√©rences:
        print("   ‚ùå Incoh√©rences d√©tect√©es :")
        for incoh√©rence in incoh√©rences:
            print(f"      - {incoh√©rence}")
    else:
        print("   ‚úÖ Aucune incoh√©rence d√©tect√©e")
    
    # 6. Solutions propos√©es
    print("\nüîß 6. SOLUTIONS PROPOS√âES:")
    
    if incoh√©rences:
        print("   üö® Probl√®mes identifi√©s :")
        print("      - Configuration incoh√©rente entre les syst√®mes")
        print("      - L'interface peut utiliser config.py au lieu de matelas_config.json")
        
        print("\n   üîß Solutions :")
        print("      1. Forcer la coh√©rence dans matelas_config.json")
        print("      2. Modifier config.py pour utiliser ollama par d√©faut")
        print("      3. V√©rifier que l'interface lit matelas_config.json")
        
    else:
        print("   ‚úÖ Configuration coh√©rente")
        print("   üöÄ Pr√™t pour tester l'application")
    
    return len(incoh√©rences) == 0

def corriger_coherence():
    """Corrige les incoh√©rences de configuration"""
    
    print("\nüîß CORRECTION DE LA COH√âRENCE:")
    
    # 1. Corriger matelas_config.json
    print("\nüìù 1. CORRECTION DE MATELAS_CONFIG.JSON:")
    
    matelas_config_file = 'matelas_config.json'
    if os.path.exists(matelas_config_file):
        try:
            with open(matelas_config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # Forcer la coh√©rence
            config['current_llm_provider'] = 'ollama'
            config['llm_provider'] = 'ollama'
            
            # Sauvegarder
            with open(matelas_config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print("   ‚úÖ matelas_config.json corrig√©")
            print("      - llm_provider: ollama")
            print("      - current_llm_provider: ollama")
            
        except Exception as e:
            print(f"   ‚ùå Erreur correction {matelas_config_file}: {e}")
    else:
        print(f"   ‚ùå {matelas_config_file} non trouv√©")
    
    # 2. Instructions de test
    print("\nüìã 2. INSTRUCTIONS DE TEST:")
    
    print("   üöÄ Maintenant testez votre application:")
    print("   1. üì± Ouvrir MatelasApp")
    print("   2. üîß V√©rifier dans 'Gestion des cl√©s API' que Ollama est s√©lectionn√©")
    print("   3. üìÑ S√©lectionner votre PDF COSTENOBLE")
    print("   4. ‚úÖ Cocher 'Utiliser l'enrichissement LLM'")
    print("   5. üöÄ Cliquer sur 'Traiter les fichiers'")
    
    print("\n   üîç V√©rifications √† faire:")
    print("   - Dans les logs: 'ollama run gpt-oss:20b' (pas openrouter)")
    print("   - Provider affich√©: ollama (pas openrouter)")
    print("   - Parsing JSON r√©ussi")
    print("   - Dimensions extraites correctement")
    print("   - Fourgon d√©tect√© et rempli")
    print("   - Jumeaux d√©tect√©s et logique appliqu√©e")
    print("   - Fichiers Excel g√©n√©r√©s avec succ√®s")

if __name__ == "__main__":
    print("üîç V√©rification de la coh√©rence des configurations")
    
    # V√©rification principale
    coh√©rent = verifier_coherence_configuration()
    
    # Correction si n√©cessaire
    if not coh√©rent:
        corriger_coherence()
    
    if coh√©rent:
        print("\nüéØ R√âSUM√â DE LA V√âRIFICATION:")
        print("‚úÖ Configuration coh√©rente")
        print("üöÄ Pr√™t pour tester l'application")
    else:
        print("\nüéØ R√âSUM√â DE LA V√âRIFICATION:")
        print("‚ùå Incoh√©rences d√©tect√©es et corrig√©es")
        print("üîß Configuration harmonis√©e")
        print("üöÄ Pr√™t pour tester l'application")
    
    print("\n=== FIN DE LA V√âRIFICATION DE COH√âRENCE ===")

