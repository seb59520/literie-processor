#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de test pour v√©rifier les nouvelles fonctionnalit√©s de temp√©rature et gestion Ollama
"""

import sys
import os
import subprocess
import json

def test_temperature_explanations():
    """Teste les explications de temp√©rature"""
    print("üå°Ô∏è Test des explications de temp√©rature")
    print("=" * 50)
    
    temperature_values = [0.0, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0]
    
    for temp in temperature_values:
        if temp == 0.0:
            explanation = "D√©terministe - R√©ponses coh√©rentes et pr√©visibles"
        elif temp <= 0.3:
            explanation = "Faible cr√©ativit√© - R√©ponses structur√©es et pr√©cises"
        elif temp <= 0.7:
            explanation = "Cr√©ativit√© mod√©r√©e - √âquilibr√© entre pr√©cision et cr√©ativit√©"
        elif temp <= 1.0:
            explanation = "Cr√©ativit√© √©lev√©e - R√©ponses vari√©es et originales"
        else:
            explanation = "Tr√®s cr√©atif - R√©ponses tr√®s vari√©es et impr√©visibles"
        
        print(f"Temp√©rature {temp}: {explanation}")
    
    print("\n‚úÖ Test des explications de temp√©rature termin√©")

def test_ollama_availability():
    """Teste la disponibilit√© d'Ollama"""
    print("\nü§ñ Test de disponibilit√© d'Ollama")
    print("=" * 50)
    
    try:
        # V√©rifier si Ollama est install√©
        result = subprocess.run(
            ["ollama", "--version"],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            print(f"‚úÖ Ollama install√©: {result.stdout.strip()}")
            
            # Tester la commande list
            list_result = subprocess.run(
                ["ollama", "list", "--json"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if list_result.returncode == 0:
                try:
                    models_data = json.loads(list_result.stdout)
                    if isinstance(models_data, list):
                        model_names = [model.get('name', '') for model in models_data if model.get('name')]
                        print(f"‚úÖ {len(model_names)} mod√®les Ollama disponibles:")
                        for model in model_names:
                            print(f"   ‚Ä¢ {model}")
                    else:
                        print("‚ö†Ô∏è Format de r√©ponse Ollama inattendu")
                except json.JSONDecodeError:
                    print("‚ö†Ô∏è Erreur de parsing JSON de la liste des mod√®les")
            else:
                print("‚ö†Ô∏è Erreur lors de la liste des mod√®les Ollama")
        else:
            print("‚ùå Ollama install√© mais erreur de version")
            
    except FileNotFoundError:
        print("‚ùå Ollama non install√© ou non trouv√© dans le PATH")
        print("üí° Pour installer Ollama: https://ollama.ai")
    except subprocess.TimeoutExpired:
        print("‚ö†Ô∏è Timeout lors de la v√©rification d'Ollama")
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification d'Ollama: {e}")

def test_application_features():
    """Teste les fonctionnalit√©s de l'application"""
    print("\nüß™ Test des fonctionnalit√©s de l'application")
    print("=" * 50)
    
    # V√©rifier que le fichier modifi√© existe
    if os.path.exists("test_llm_prompt.py"):
        print("‚úÖ Fichier test_llm_prompt.py trouv√©")
        
        # V√©rifier les nouvelles fonctionnalit√©s
        with open("test_llm_prompt.py", "r", encoding="utf-8") as f:
            content = f.read()
            
        features_to_check = [
            ("Explications de temp√©rature", "temp_explanation"),
            ("Bouton d'ajout Ollama", "add_ollama_btn"),
            ("Bouton de rafra√Æchissement Ollama", "refresh_ollama_btn"),
            ("Thread de t√©l√©chargement Ollama", "OllamaDownloadThread"),
            ("Gestion des mod√®les Ollama", "load_ollama_models"),
            ("Mise √† jour de temp√©rature", "on_temperature_changed")
        ]
        
        for feature_name, feature_code in features_to_check:
            if feature_code in content:
                print(f"‚úÖ {feature_name} - Impl√©ment√©")
            else:
                print(f"‚ùå {feature_name} - Manquant")
    else:
        print("‚ùå Fichier test_llm_prompt.py non trouv√©")

def main():
    """Fonction principale de test"""
    print("üß™ Test des nouvelles fonctionnalit√©s - Temp√©rature et Ollama")
    print("=" * 70)
    
    test_temperature_explanations()
    test_ollama_availability()
    test_application_features()
    
    print("\nüéâ Tests termin√©s !")
    print("\nüìã R√©sum√© des nouvelles fonctionnalit√©s:")
    print("   ‚Ä¢ Explications d√©taill√©es de la temp√©rature")
    print("   ‚Ä¢ Gestion avanc√©e des mod√®les Ollama")
    print("   ‚Ä¢ Ajout de nouveaux mod√®les Ollama")
    print("   ‚Ä¢ Rafra√Æchissement de la liste des mod√®les")
    print("   ‚Ä¢ T√©l√©chargement automatique des mod√®les")
    print("   ‚Ä¢ Interface utilisateur am√©lior√©e")

if __name__ == "__main__":
    main() 