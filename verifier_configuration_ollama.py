#!/usr/bin/env python3
"""
VÃ©rification de la configuration Ollama
"""

import json
import os

def verifier_configuration_ollama():
    """VÃ©rifie la configuration Ollama actuelle"""
    
    print("=== VÃ‰RIFICATION DE LA CONFIGURATION OLLAMA ===")
    
    # 1. VÃ©rifier le fichier de configuration
    print("\nğŸ“ 1. VÃ©rification du fichier de configuration:")
    
    config_files = [
        'matelas_config.json',
        'config.py',
        'backend_interface.py'
    ]
    
    for config_file in config_files:
        if os.path.exists(config_file):
            print(f"   âœ… {config_file} trouvÃ©")
        else:
            print(f"   âŒ {config_file} non trouvÃ©")
    
    # 2. Lire la configuration principale
    print("\nğŸ” 2. Lecture de la configuration principale:")
    
    if os.path.exists('matelas_config.json'):
        try:
            with open('matelas_config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            print("   âœ… Configuration chargÃ©e avec succÃ¨s")
            
            # VÃ©rifier la configuration LLM
            if 'llm_provider' in config:
                print(f"   ğŸ”§ Provider LLM configurÃ©: {config['llm_provider']}")
            else:
                print("   âŒ Provider LLM non configurÃ©")
            
            # VÃ©rifier la configuration Ollama
            if 'ollama' in config:
                ollama_config = config['ollama']
                print("   ğŸ“Š Configuration Ollama:")
                for key, value in ollama_config.items():
                    print(f"      - {key}: {value}")
            else:
                print("   âŒ Configuration Ollama non trouvÃ©e")
                
        except Exception as e:
            print(f"   âŒ Erreur lecture configuration: {e}")
    else:
        print("   âŒ Fichier matelas_config.json non trouvÃ©")
    
    # 3. VÃ©rifier les modÃ¨les disponibles
    print("\nğŸ” 3. VÃ©rification des modÃ¨les Ollama disponibles:")
    
    try:
        import subprocess
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("   âœ… ModÃ¨les Ollama disponibles:")
            lines = result.stdout.strip().split('\n')
            for line in lines[1:]:  # Ignorer l'en-tÃªte
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 2:
                        model_name = parts[0]
                        model_size = parts[1]
                        print(f"      - {model_name} ({model_size})")
        else:
            print("   âŒ Erreur lors de la liste des modÃ¨les")
            print(f"      Erreur: {result.stderr}")
            
    except Exception as e:
        print(f"   âŒ Impossible d'exÃ©cuter 'ollama list': {e}")
    
    # 4. VÃ©rifier le modÃ¨le par dÃ©faut
    print("\nğŸ” 4. VÃ©rification du modÃ¨le par dÃ©faut:")
    
    try:
        result = subprocess.run(['ollama', 'ps'], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("   âœ… ModÃ¨les Ollama en cours d'exÃ©cution:")
            lines = result.stdout.strip().split('\n')
            for line in lines[1:]:  # Ignorer l'en-tÃªte
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 3:
                        model_name = parts[0]
                        model_id = parts[1]
                        status = parts[2]
                        print(f"      - {model_name} (ID: {model_id}, Status: {status})")
        else:
            print("   â„¹ï¸ Aucun modÃ¨le Ollama en cours d'exÃ©cution")
            
    except Exception as e:
        print(f"   âŒ Impossible de vÃ©rifier les modÃ¨les en cours: {e}")
    
    # 5. Analyse du problÃ¨me dans le log
    print("\nğŸ” 5. Analyse du problÃ¨me dans le log:")
    
    print("   ğŸ“Š ProblÃ¨me dÃ©tectÃ©:")
    print("      - Le log montre: 'ollama run mistral:latest'")
    print("      - Cela signifie que l'application utilise mistral:latest")
    print("      - Au lieu du modÃ¨le que vous aviez configurÃ©")
    
    print("\n   ğŸ¯ Causes possibles:")
    print("      1. âŒ ModÃ¨le par dÃ©faut non configurÃ©")
    print("      2. âŒ Configuration Ollama manquante")
    print("      3. âŒ Fallback automatique sur mistral:latest")
    
    # 6. Solutions proposÃ©es
    print("\nğŸ”§ 6. Solutions proposÃ©es:")
    
    print("   ğŸš€ Solution 1: Configurer le modÃ¨le par dÃ©faut")
    print("      - Dans l'application: Gestion des clÃ©s API")
    print("      - SÃ©lectionner Ollama et configurer le modÃ¨le")
    print("      - Sauvegarder la configuration")
    
    print("\n   ğŸš€ Solution 2: VÃ©rifier la configuration Ollama")
    print("      - S'assurer qu'un modÃ¨le spÃ©cifique est configurÃ©")
    print("      - Ã‰viter le fallback sur mistral:latest")
    
    print("\n   ğŸš€ Solution 3: Tester avec un modÃ¨le spÃ©cifique")
    print("      - Configurer explicitement un modÃ¨le (ex: llama3.2)")
    print("      - Tester le traitement avec ce modÃ¨le")
    
    return True

def instructions_configuration():
    """Donne les instructions pour configurer Ollama"""
    
    print("\nğŸ“‹ INSTRUCTIONS DE CONFIGURATION OLLAMA:")
    
    print("   1. ğŸ“± Ouvrir l'application MatelasApp")
    print("   2. ğŸ”§ Aller dans 'Gestion des clÃ©s API'")
    print("   3. ğŸ¯ SÃ©lectionner l'onglet 'Ollama'")
    print("   4. ğŸ“ Configurer le modÃ¨le par dÃ©faut:")
    print("      - ModÃ¨le: (ex: llama3.2, codellama, etc.)")
    print("      - Pas de clÃ© API nÃ©cessaire")
    print("   5. ğŸ’¾ Sauvegarder la configuration")
    print("   6. ğŸš€ Tester avec votre PDF COSTENOBLE")
    
    print("\n   ğŸ” Points de vÃ©rification:")
    print("      - ModÃ¨le Ollama configurÃ© (pas mistral:latest)")
    print("      - Configuration sauvegardÃ©e")
    print("      - Option LLM activÃ©e lors du test")

if __name__ == "__main__":
    print("ğŸš€ VÃ©rification de la configuration Ollama")
    
    # VÃ©rification principale
    success = verifier_configuration_ollama()
    
    # Instructions de configuration
    instructions_configuration()
    
    if success:
        print("\nğŸ¯ RÃ‰SUMÃ‰:")
        print("âœ… La configuration Ollama a Ã©tÃ© vÃ©rifiÃ©e")
        print("ğŸ”§ Des solutions ont Ã©tÃ© proposÃ©es")
        print("ğŸš€ Suivez les instructions pour configurer le bon modÃ¨le")
    else:
        print("\nâŒ ProblÃ¨me dÃ©tectÃ© dans la vÃ©rification")
    
    print("\n=== FIN DE LA VÃ‰RIFICATION ===")

