#!/usr/bin/env python3
"""
Test de la configuration Ollama sans cl√© API
"""

import sys
import os
import json

# Ajouter le r√©pertoire backend au path
sys.path.append('backend')

from config import config
from llm_provider import OllamaProvider, LLMProviderManager

def test_ollama_config():
    """Test de la configuration Ollama"""
    
    print("=== TEST CONFIGURATION OLLAMA ===")
    
    # 1. V√©rifier la configuration actuelle
    print("\nüìã √âtape 1: V√©rification de la configuration")
    
    current_provider = config.get_current_llm_provider()
    print(f"‚úÖ Provider actuel: {current_provider}")
    
    ollama_base_url = config.get_ollama_base_url()
    print(f"‚úÖ URL Ollama: {ollama_base_url or 'localhost:11434 (d√©faut)'}")
    
    ollama_model = config.get_llm_model("ollama")
    print(f"‚úÖ Mod√®le Ollama: {ollama_model or 'mistral:latest (d√©faut)'}")
    
    # 2. V√©rifier que Ollama est configur√© comme provider actuel
    print("\nüìã √âtape 2: V√©rification du provider Ollama")
    
    if current_provider == "ollama":
        print("‚úÖ Ollama est bien configur√© comme provider actuel")
    else:
        print(f"‚ùå Provider actuel: {current_provider} (devrait √™tre 'ollama')")
        print("üîß Correction automatique...")
        config.set_current_llm_provider("ollama")
        print("‚úÖ Provider corrig√©: ollama")
    
    # 3. Test de cr√©ation du provider Ollama
    print("\nüìã √âtape 3: Test de cr√©ation du provider Ollama")
    
    try:
        # Cr√©er directement le provider Ollama
        ollama_provider = OllamaProvider()
        print("‚úÖ Provider Ollama cr√©√© avec succ√®s")
        print(f"  üìç URL de base: {ollama_provider.base_url}")
        
        # Test de connexion
        print("\nüìã Test de connexion Ollama...")
        if ollama_provider.test_connection():
            print("‚úÖ Connexion Ollama r√©ussie")
        else:
            print("‚ùå Connexion Ollama √©chou√©e")
            print("üí° V√©rifiez que Ollama est lanc√©: ollama serve")
            
    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation du provider Ollama: {e}")
        return False
    
    # 4. Test avec le gestionnaire de providers
    print("\nüìã √âtape 4: Test avec le gestionnaire de providers")
    
    try:
        manager = LLMProviderManager()
        print("‚úÖ Gestionnaire de providers cr√©√©")
        
        # D√©finir Ollama comme provider
        manager.set_provider("ollama")
        print("‚úÖ Provider d√©fini: ollama")
        
        # Obtenir l'instance du provider
        provider_instance = manager.get_provider_instance()
        if provider_instance:
            print("‚úÖ Instance du provider Ollama obtenue")
            print(f"  üìç Type: {type(provider_instance)}")
        else:
            print("‚ùå Impossible d'obtenir l'instance du provider")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur avec le gestionnaire de providers: {e}")
        return False
    
    # 5. Test de la configuration des mod√®les
    print("\nüìã √âtape 5: Test de la configuration des mod√®les")
    
    try:
        # V√©rifier le mod√®le configur√©
        model = config.get_llm_model("ollama") or "mistral:latest"
        print(f"‚úÖ Mod√®le configur√©: {model}")
        
        # D√©finir un mod√®le personnalis√©
        test_model = "llama2:latest"
        config.set_llm_model("ollama", test_model)
        print(f"‚úÖ Nouveau mod√®le d√©fini: {test_model}")
        
        # V√©rifier que le mod√®le a √©t√© sauvegard√©
        saved_model = config.get_llm_model("ollama")
        if saved_model == test_model:
            print("‚úÖ Mod√®le sauvegard√© avec succ√®s")
        else:
            print(f"‚ùå Mod√®le non sauvegard√©: {saved_model}")
            
        # Restaurer le mod√®le original
        config.set_llm_model("ollama", model)
        print(f"‚úÖ Mod√®le original restaur√©: {model}")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la configuration des mod√®les: {e}")
        return False
    
    # 6. Test de la configuration de l'URL
    print("\nüìã √âtape 6: Test de la configuration de l'URL")
    
    try:
        # D√©finir une URL personnalis√©e
        test_url = "http://localhost:11434"
        config.set_ollama_base_url(test_url)
        print(f"‚úÖ URL d√©finie: {test_url}")
        
        # V√©rifier que l'URL a √©t√© sauvegard√©e
        saved_url = config.get_ollama_base_url()
        if saved_url == test_url:
            print("‚úÖ URL sauvegard√©e avec succ√®s")
        else:
            print(f"‚ùå URL non sauvegard√©e: {saved_url}")
            
        # Restaurer l'URL par d√©faut
        config.set_ollama_base_url("")
        print("‚úÖ URL par d√©faut restaur√©e")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la configuration de l'URL: {e}")
        return False
    
    print("\nüéâ Tous les tests Ollama sont pass√©s avec succ√®s !")
    print("‚úÖ La configuration Ollama fonctionne correctement")
    print("‚úÖ Aucune cl√© API n'est requise")
    print("‚úÖ Les mod√®les et URLs sont configurables")
    
    return True

def test_ollama_vs_other_providers():
    """Test de comparaison entre Ollama et les autres providers"""
    
    print("\n=== COMPARAISON OLLAMA vs AUTRES PROVIDERS ===")
    
    # V√©rifier les cl√©s API des autres providers
    providers = ["openrouter", "openai", "anthropic", "gemini", "mistral"]
    
    for provider in providers:
        api_key = config.get_llm_api_key(provider)
        if api_key:
            print(f"‚úÖ {provider}: Cl√© API configur√©e ({len(api_key)} caract√®res)")
        else:
            print(f"‚ùå {provider}: Aucune cl√© API configur√©e")
    
    # V√©rifier qu'Ollama n'a pas de cl√© API
    ollama_key = config.get_llm_api_key("ollama")
    if not ollama_key:
        print("‚úÖ Ollama: Aucune cl√© API (normal)")
    else:
        print(f"‚ö†Ô∏è Ollama: Cl√© API trouv√©e ({ollama_key}) - devrait √™tre vide")

if __name__ == "__main__":
    print("üöÄ D√©marrage des tests de configuration Ollama")
    
    # Test principal
    success = test_ollama_config()
    
    # Test de comparaison
    test_ollama_vs_other_providers()
    
    if success:
        print("\nüéâ Configuration Ollama valid√©e !")
        print("‚úÖ Ollama fonctionne sans cl√© API")
        print("‚úÖ La configuration est persistante")
        print("‚úÖ Les mod√®les et URLs sont configurables")
    else:
        print("\n‚ùå Probl√®mes d√©tect√©s avec la configuration Ollama")
        print("üîß V√©rifiez les erreurs ci-dessus")
    
    print("\n=== FIN DES TESTS ===")

